Google Cloud Essentials

Interactive Diagram https://linuxacademy.com/cp/guides/download/refsheets/guides/refsheets/google-essentials_1529985391.html

App Engine Fundamentals
PaaS - Platform as a Service
No server setup or provisioning with minimal management required
Automatic scaling and load-balancing
Great for websites, mobile apps, and line of business apps
Standard Environment         Flexible Environment
  Earliest implementation      Recently Introduced
  More proprietary             Standardized on Docker
  Limited languages and access Broader language/version use
  Faster instance spin up      slower instance spin up
  Less expensive               more expensive

Compute Engine Fundamentals
IaaS - Infrastructure as a Service
Scalable, high-performance virtual machines (VMs)
Numerous configurations, completely customizable
Run public disk images or private images
Various storage options
Optionally, works with containers
Virtual Private Network (VPC) support
Default and custom firewalls
Complete routing support

Kubernetes Engine Fundamentals
Managed, orchestrated environment for containerized applications
Uses Compute Engine to form cluster
Relies on open source Kubernetes cluster management system
Currently only Docker containers are supported
Benefits
  Load-balancing integrated
  Node pools supported
  Automatic cluster and node scaling
  Automatic upgrades
  Automatic repair based on health reports
  Automatic logging and monitoring with Stackdriver

Cloud Functions Fundamentals
Serverless Environment for executing code and connecting cloud services
Fully managed zero infrastructure or management requirements
JavaScript functions in a Node.js wrapper
Triggers:
  HTTP request
  Cloud Storage
  Pub/Sub Event
Use cases:
  Webhooks - Respond to any HTTP request
  Data & Image Processing - Validate/transform data or manipulate images
  Mobile Back end - React to storage, authentication or data events
  Internet of Things - Respond to Pub/Sub messages from devices

Hands-On Lab: Deploying an App Engine Application
You’ve been asked to deploy the latest version of your company’s new app, LA Music Gallery, on Google Cloud App Engine.
The app allows a user to enter details about a music album, including the name of both the artist and the album,
as well as the HTML and CSS to present the app. To accomplish this, the app, written in Python,
stores data in Cloud Datastore using App Engine’s standard environment.

You’ll need to accomplish the following steps to complete your task:

Ensure required APIs are available.
Clone a GitHub repo to acquire the files.
Properly configure the application by modifying a YAML file.
Deploy the app to App Engine in the proper region.
Test the app.

Enable APIs and clone GitHub repository.
keyboard_arrow_up
From the main navigation, visit the APIs & Services > Libraries page, and search for "Datastore".
Select the Cloud Datastore API.
If the API is not available, click Enable.
Activate the Cloud Shell.
When it spins up, create the necessary bucket (bucket name must be unique):
 gsutil mb -c regional -l us-east1 gs://[BUCKET_NAME]
 gsutil iam ch allUsers:objectViewer gs://[BUCKET_NAME]
Clone the GitHub repository:
 git clone https://github.com/linuxacademy/content-gc-essentials
Change directory to the content-gc-essentials/app-engine-lab folder.

Configure `config.py` file.
keyboard_arrow_up
From the Cloud Shell Editor, open config.py in the app-engine-lab folder.
Change PROJECT_ID to the current project, as shown in the Cloud Shell.
Change the "CLOUD_STORAGE_BUCKET" variable value to your unique bucket name.
Save the file.
check_circle
Deploy the app.
keyboard_arrow_up
In the Cloud Shell, enter the following code:
gcloud app deploy
When prompted, choose the nearest region.
check_circle
Test the app.
keyboard_arrow_up
In the Cloud Shell, enter the following code:
gcloud app browse
If the browser window does not open, click the generated link.

Hands-On Lab: Triggering a Cloud Function with Cloud Pub/Sub
You’ve been asked to lay the groundwork for one aspect of the new app that
relies on code being executed when a message is received from any number of outlets.
To do this, you’ll need to establish a Cloud Pub/Sub topic and then create an example
Cloud Function that uses the topic as a trigger. Naturally, you’ll need to confirm
the Cloud Function has successfully executed.

You’ll need to accomplish the following steps to complete your task:

Enable APIs.
Create Pub/Sub topic.
Create Cloud Function.
Publish message on topic.
Confirm Cloud Function executed via logs.
Trigger directly via the command line.
Confirm Cloud Function executed via logs.
Send message to topic from command line.
Confirm Cloud Function executed via logs.

Enable required APIs.
keyboard_arrow_up
Use the API Library to find and enable the Cloud Pub/Sub API.
Use the API Library to find and enable the Cloud Functions API.
check_circle
Create Pub/Sub topic.
keyboard_arrow_up
From the main console navigation, go to Cloud Pub/Sub.
Click Create a topic.
In the dialog, enter a name greetings for the topic.
Click Create.

Create a Cloud Function.
keyboard_arrow_up
From the main console, go to Cloud Functions.
Click Create function.
Configure the function with the following values:
Name: la-pubsub-function
Trigger: Cloud Pub/Sub
Topic: greetings
Source code: Inline editor
Runtime: Python 3.7
In the main.py field, enter the following code:

import base64

def greetings_pubsub(data, context):

    if 'data' in data:
        name = base64.b64decode(data['data']).decode('utf-8')
    else:
        name = 'folks'
    print('Greetings {} from Linux Academy!'.format(name))
Set Function to Execute to greetings_pubsub.

Click Create.

Publish message to topic from console.
keyboard_arrow_up
Click the newly created Cloud Function name.
Switch to Trigger.
Click the topic link to go to the Cloud Pub/Sub topic.
From the Topic page, click Publish Message.
In the Message field, enter everyone around the world.
Click Publish.
check_circle
Confirm Cloud Function execution.
keyboard_arrow_up
Return to the Cloud Functions dashboard.
Click the Cloud Function's name.
From the Cloud Function navigation, click View Logs.
Locate the most recent log.
Confirm function execution.
check_circle
Trigger Cloud Function directly from command line.
keyboard_arrow_up
Click Activate Cloud Shell from the top row of the console.
In the Cloud Shell, enter the following code:

DATA=$(printf 'my friends' | base64)
gcloud functions call la-pubsub-function --data '{"data":"'$DATA'"}'
After a moment, refresh the logs page and confirm the Cloud Function execution.

Publish message to topic from command line.
keyboard_arrow_up
In the Cloud Shell, enter the following command:

 gcloud pubsub topics publish greetings --message "y'all"
After a moment, refresh the log page to confirm the function has executed.


Hands-On Lab: Working with Compute Engine Windows Instances
You’ve been tasked with getting a Windows web server up and running on a Compute Engine instance.
To accomplish this, you’ll need to create the appropriate instance,
use RDP to set up necessary software, and create a home page to test the server.

You’ll need to accomplish the following steps to complete your task:
Create a Compute Engine VM instance.
keyboard_arrow_up
From the main navigation, choose Compute Engine > VMs.
In the VM Instances area, click Create.
With New VM instance chosen from the options on the left, configure your instance:
In the name field, provide a relevant name using hyphens, like la-windows-1.
Keep the suggested Region and Zone.
In the Boot Disk section, click Change and select Windows Server 2019 Datacenter, Server with Desktop Experience from the list; click Select.
Under Firewall, choose the Allow HTTP traffic option.
Click Create.
check_circle
Set Windows password.
keyboard_arrow_up
From the RDP options, click Set Windows password.
In the dialog, confirm your username is correct and click Set.
Copy the supplied password and click Close.
check_circle
Launch RDP window.
keyboard_arrow_up
Launch the RDP window by using one of the following methods:
If you're on a Windows system, click RDP.
If you're on a Mac using Chrome in a standard window, first install the Chrome RDP extension, and then click RDP.
If you're on a Mac using another browser or Incognito window, from the App Store,
download and install the latest version of the Microsoft Remote Desktop app.
Then, choose Download the RDP file from the RDP options and open the file.

Install IIS.
keyboard_arrow_up
From the Windows Start menu, right-click on Windows Powershell and choose Run as administrator.
In the PowerShell window, enter the following commands:
 import-module servermanager
 add-windowsfeature web-server -includeallsubfeature
 echo '<!doctype html><html><body><h1>Greetings from Linux Academy!</h1></body></html>' > C:\inetpub\wwwroot\index.html
check_circle
Test your page.
keyboard_arrow_up
From the Compute Engine VM page, click the External link for your Windows VM instance.
Review the page in your browser.


Cloud Identity Fundamentals
  Identity as a Service - IDaaS
  Originally G-Suite based with separate Google Admin
  Intergrated with Google Cloud
  Manage resources hierarchically
  Assign unmanaged accounts to project
  Allow single sign-on
  Hierarchy- Organization->Folders->1 or more Projects->1 or more Resources per project
  Policy inheritance

Cloud IAM Fundamentals
  Unified resource access managemenet system
  For both users and services
  3 main components: policies, roles, resources
  Policy: Who can do what on which resources
  Roles: List of permissions assigned to identities/members
  Identities:
    Google account(managed account), unmanaged account
    Service account
    Google Group, G-Suite Domain, Cloud Identity Domain
  Resources:
    Projects and folders
    Cloud services(Compute Engine, Cloud Storage, Pub/Sub, etc)
    Aspects of those services(instances, buckets, topics, etc.)

Cloud KMS
  Cryptographic key management system
  Keys are used to encrypt/decrypt files
  Hierarchical, like IAM
    Project>Location>Key Ring>Key>Key Version
  Cloud KMS is a project-based resource
  Specify locations:
    Regional, Multi-Regional, or Global
  Key ring is a collection of keys in a specified location for a specific project
  Individual keys inherit permissions from keyring
  Different key versions have different encryption/decryption values
  Key version states: Enabled, Disabled, Scheduled for Destruction, Destroyed
  Key versions can be rotated regularly and automatically or manually

Cloud IAP
  Application-level authorization service
  Based on internal Google enterprise security model, BeyondCorp
  Supplements network-level firewalls
  Ideal for line of business apps
  No VPN needed:
    Straight-forward implementation for administrators
    Simple to use for remote workers
  No additional charge

---

Cloud Storage Fundamentals
  Binary large object(BLOB) storage
  Images, videos, audio files, documents, static webistes, etc.
  Automatic data encryption at rest and decryption on delivery
  Primary container: buckets
    Project-based
    Globally unique ID
    Specific location
    Set class for optimum price/performance
      Multi-regional - highes availability, most frequently accessed
      Regional - routinely accessed, best for analytics
      Nearline - infrequently accessed, used for archival and data backup
      Coldline - least accessed, lowest cost, typically for disaster recovery

Cloud Datastor Fundamentals
  NoSQL document database for semi-structured data
  Key Features:
    ACID transactions
    Highly available and scalable
    Multiple access options, i.e. Console, JSON API, and open source clients
    SQL-like language, GQL
  Structure - similar to traditional, but more flexible (schemaless):
    Kinds - like tables
    Entity - like row, but can have different properties
    Property - like field, but can multiple properties possible
    Key - like primary index
    Supports optional ancestors and children
  Uses: product catalogs, user profiles, ACID transactions, etc.

Cloud SQL Fundamentals
  Fully managed relational database service
  Supports PostgresSQL 9.6
  Supports MySQL 5.5(1st generation) and 5.6 or 5.7(2nd generation)
  Robust scalability
  Automatic replication and backup
  Highly configurable SQL instances
  Data automatically encrypted
  Default firewalls for each instance
  Full integration with Google Cloud services

Cloud Bigtable Fundamentals
  Fully managed, massively scalable NoSQL database service for big data
  Used for Gmail, Google Search, Maps & Analytics as well as eBay and Spotify
  Although also NoSQL, different from Cloud Datastore
    Wide column database vs document database
    No SQL-like language available
    Single key per row
  Capable of holding hundreds of petabytes of information
  Columns wide enough for entire web pages or satellite imagery
  Consistent low latency and high throughput
  Dynamically change cluster size without stopping and restarting
  Use cases: graph data, financial data, IoT data, marketing data, etc.

Open a Google Cloud Shell instance to use bigtable
gcloud components update
sudo gcloud components install cbt
echo project = laca-international > ~/.cbtrc
echo instance - laca-international-cbt >> ~/.cbtrc
cbt createtable laca-table
cbt ls
cbt createfamily laca-table cf1
cbt set laca-table r1 cf1:c1=laca-int-airport
cbt read laca-table

Cloud Spanner Fundamentals
  Fully managed, enterprise-grade, relational database service
  Is to Cloud SQL as Cloud Bigtable is to Cloud Datastore
  Scales horizontally like NoSQL databases
  High availability(99.999% uptime) with strong consistency
  Industry standard SQL support
  Supports data definition language(DDL)
  Client libraries: C#, Go, Java, Node,js, PHP, Python, and Ruby
  Full console support
  Use cases: call centers, financial trading, telecom, transportation, etc.

Cloud MemoryStore Fundamentals
  Fully managed, in-memory datastore service
  Recently released
  Redis protocol compatible
  Extrememly low latency: sub-millisecond
  As-needed scaling, up to 300 GB instance
  Connect with App Engine, Compute Engine, and Kubernetes Engine
  Service tiers:
    Basic - default, for basic caching
    Standard - for highly available Redis instance
  Use cases: caching layer in gaming and analytical pipelines, stream processing

Hands-On Lab: Setting Cloud Storage Lifecycle Rules
Your entire company is looking for ways to lower expenses, and you’ve been asked to set up a series of
Cloud Storage lifecycle rules for a particular bucket that will move the objects within it from a
Regional storage class to Nearline after six months, then to Coldline after a year, and finally to
delete the objects after two years. You decide to use both the console and the command line to achieve these goals.

You’ll need to accomplish the following steps to complete your task:

Create a Cloud Storage bucket.
Establish lifecycle rules in the console.
After 180 days, set to Nearline.
After 365 days, set to Coldline.
From command line, get lifecycle rules.
Set lifecycle rule from command line using JSON file.

Create a Cloud Storage bucket.
From the Google Cloud console main navigation, choose Cloud Storage.
Click Create bucket.
Name the bucket uniquely and click Continue
In the Location Type section, select Region and click Continue.
Click Create.

Define first lifecycle rule.
From the Cloud Storage browser page, click None in the Lifecycle column for the bucket just created.
Click Add rule.
Under Select object conditions, set the following:
Age: 180
Storage class: Regional, Standard
Click Continue.
Under Select action, choose Set to Nearline.
Click Continue.
Click Save.

Define second lifecycle rule.
From the Cloud Storage browser page, click Enabled in the Lifecycle column.
Click Add rule.
Under Select object conditions, set the following:
Age: 365
Storage class: Nearline
Click Continue.
Under Select action, choose Set to Coldline.
Click Continue.
Click Save.

From command line, get lifecycle rules.
Click Activate Cloud Shell.

In the Cloud Shell, enter the following code:

gsutil lifecycle get gs://[BUCKETNAME]
Review output.

Set lifecycle rule with JSON file.
Clone a repo and change to the lab's directory:

 git clone https://github.com/linuxacademy/content-gc-essentials
 cd content-gc-essentials/cloud-storage-lifecycle-lab
Review file in editor.

In the Cloud Shell, enter the following code:

 gsutil lifecycle set delete-after-two-years.json gs://[BUCKET_NAME]
Confirm the lifecycle rule has been added in the console.
----

Hands-On Lab: Managing Google Cloud SQL Instances
You’ve been asked to take control of the team's Cloud SQL initiative.
To fully grasp how the platform works, you’ll need to understand basic Cloud SQL instance management tasks.
You’ll need to know how to create instances, as well as clone, start, stop, restart, and delete them.

You’ll need to accomplish the following steps to complete your task:
Create instance.
From the main navigation, click Cloud SQL.
Click Create Instance.
Choose your database engine.
Enter an instance ID.
Provide a password.
Click Create.

Create a database.
Select the recently created Cloud SQL instance.
Choose Create database.
Name the database and leave the other fields at their default values.
Click Create.

Clone the instance.
From the Instance details page, choose Create clone.
Enter a new name for the clone if desired.
Choose Clone latest state of instance.
Click Create clone.

Stop and start an instance.
Select the first instance created.
Click Stop.
After the instance has stopped, click Start.

Restart an instance.
Select the cloned instance.
Click Restart.

Delete an instance.
Select the cloned instance.
Click Delete.
Enter the name of the instance in the dialog to confirm the deletion.
Click Delete.
----

Hands-On Lab: Exploring Cloud Firestore in Datastore Mode
You’ve been charged with setting up a non-relational database for LA International to track flights,
arrival times, and on-time status. You decide to use Cloud Firestore in Datastore mode because of its
flexible NoSQL structure. After establishing the initial data, you need to run several queries in GQL
to ensure all the necessary properties are searchable.

You’ll need to accomplish the following steps to complete your task:
Enable APIs.
From the main navigation, click APIs and Libraries > Library.
Search for Datastore.
Click Enable.

Create the database.
From the main navigation, click Datastore.
Choose Datastore Mode.
Select your closest location.
Click Create database.

Define entities.
For Kind, enter Flights.
Click Create Entity.
Click Add property for each of the following entries, of the specified type:
Airline: String
Flight Number: Integer
Arrival: Data and Time
OnTime: Boolean
Click Save.
Repeat steps 3 and 4 twice more with different values.
For the final entry, add another property:
Note: Text
Click Save.

Query the data.
Switch to Query by GQL.
In the Query field, enter the following:
 SELECT * FROM Flights
Click Run Query.
In the Query field, enter the following:
 SELECT * FROM Flights WHERE OnTime = false
Click Run Query.
Review results.
----
Hands-On Lab: Connecting to Cloud Bigtable with cbt
In preparation for an Internet of Things project involving a great deal of streaming real-time data,
you’ve been asked to get up to speed on Cloud Bigtable. You’ll need to learn how to create instances
and then, using the command line cbt commands, define the initial table and its structure.

You’ll need to accomplish the following steps to complete your task:
Enable API.
From the main navigation, choose APIs & Services > Library.
In the search field, enter Bigtable.
Select the Cloud Bigtable Admin API card.
Click Enable.

Create Bigtable instance.
From the main navigation, choose Bigtable in the Storage section.
Choose Create instance.
Set the following fields:
Instance Name: la-data-cbt
Instance type: Development
Storage type: HDD
Cluster Region: us-east1
Cluster Zone: us-east1-b
Click Done and then Create.

Install and configure `cbt`.
Activate the Cloud Shell by clicking its icon in the top row.

In the Cloud Shell, enter the following:

gcloud components update
gcloud components install cbt
Configure cbt with the following commands:

echo project = [PROJECT_ID] > ~/.cbtrc
echo instance = la-data-cbt >> ~/.cbtrc

Create data table.
In the Cloud Shell, enter the following:

 cbt createtable la-table
 cbt ls

Define table structure and add data.
In the Cloud Shell, enter the following:

cbt createfamily la-table offerings
cbt set la-table r1 offerings:c1=labs
cbt read la-table

Review results.

Enter the following:

cbt set la-table r1 offerings:c2=courses
cbt read la-table

Delete Bigtable instance.
From the console, select the Bigtable instance.
Choose Delete instance.
----

Cloud BigQuery Fundamentals
  Fully managed data warehouse for big data
  Near real-time interactive analysis of massive datasets
  Analyze terabytes of data in seconds
  Standard SQL supported
  Storage and computing handled and billed separately
  Query public or commercial dataset with your own
  External services queries: Cloud Storage, Cloud Bigtable & Google Drive
  Automatic data replication
  Modify data with Data Definition Language
  Use cases: real-time inventory, predictive digital marketing, analytic events

Cloud Datafow Fundamentals
  Fully managed service for creating pipelines to process data
  Based on Apache Beam
  Processes data on multiple machines in parallel
  Handles both streaming (live) and batch (archived) data
  No instances or clusters to establish: serverless
  Easy replication of services with templates:
    No need to recompile code before processing pipeline
    Execute pipeline without dev environment and its dependencies
    Can customize exection with template parameters
    Can be executed via the console or gcloud command
  Best option if no current implementation with Apache Hadoop or Spark
  Use cases: analytical dashboards, forecasting sales trends, ELT operations

Cloud Datapro Fundamentals
  Fully managed cluster data processing service
  Compatible with Apache Hadoop, Spark, and Hive
  Move existing projects or pipelines without redevelopment
  Boasts fast cluster creation - 90 seconds vs. 5 - 30 minutes
  Can scale clusters up and down without stopping the job
  Can switch to different versions of Hadoop, Spark, and others
  Workflow templates recently added
    Create template, add job, and instantiate template
      Workflow 1: Creates cluster, runs jobs, and deletes cluster
      Workflow 2: Works with exisiting cluster and runs jobs
  Similar to Cloud Dataflow
    Both process data
    Both handle batch and streaming data

How to choose between Cloud Dataflow and Cloud Dataproc
  Do you have dependencies of specific tools/packages in the Apache Hadoop/Spark Ecosystem?
    Yes -> Cloud Dataproc
    No -> Do you favor a hands-on/DevOps approach to operations, or a hands-off/serverless one?
      DevOps -> Cloud Dataproc
      Serverless -> Cloud Dataflow

Cloud Pub/Sub Fundamentals
  Fully managed messaging middleware service
  Allows secure and highly available messages between independent apps
  Works with both Google Cloud and external services
  Full range of communication:
    One to many
    Many to one
    Many to many
  Both push and pull options
  Messages encrypted and HIPAA compliant
  Use cases: streaming data, event notifications, asynchronous workflows, etc.

Publisher==Message->Topic(->Message Storage)->Subscription->Message->Subscriber->Ack->Subscription

Cloud Datalab Fundamentals
  Interactive data analysis and machine learning environment
  Packaged as a container and runs in a VM instance
  Based on Jupyter notebooks
    Notebooks:
      Contain code, docs in markdown, and code results
      Code results can be text, image, JavaScript, or HTML
      Can be shared with team members
      Collections of cells containing code or markdown
create zone
datalab create laca-datalab
datalab connect --zone us-east1-b --port 8081 laca-datalab

Cloud Data Studio Fundamentals
  Interactive report and big data visualizer
  Creates dashboards, charts, and tables
  Connects to Cloud BigQuery, Cloud Spanner, Cloud SQL, & Cloud Storage
  Stores shareable files on Google Drive
  Basic process in three steps:
    Connect to data source
    Visualize data in report
    Share report
----
Hands-On Lab: Handling Streaming Messages with Cloud Pub/Sub
Your company has just been hired to track all the sensor data coming
from traffic sensors in a major American city. You’ve been asked to
verify that Cloud Pub/Sub is capable of handling on-going streaming data.

You’ll need to complete the following steps to accomplish your task:
Create a Topic
From the main navigation menu, select Pub/Sub > Topics.
Click Create a topic.
Enter a name for the topic, such as "la-streaming-topic".
Keep the Encryption option at its default setting.
Click Create Topic.

Create a Subscription
Drill down into the topic we just created and choose Create Subscription from the bottom of the page in the Subscriptions section.
Enter a name for the subscription, such as "la-streaming-subscription".
Set Delivery Type to Pull.
Under Retain acknowledged messages, click the Enable option.
Leave all the other options as their defaults.
Click Create.

Retrieve the Files
From the top navigation, click Activate Cloud Shell.
In the Cloud Shell, enter the following command to clone the GitHub repository:
git clone https://github.com/GoogleCloudPlatform/training-data-analyst
Change to the training-data-analyst/courses/streaming/publish directory:
cd training-data-analyst/courses/streaming/publish
Copy the data file from a Cloud Storage bucket:
gsutil cp gs://cloud-training-demos/sandiego/sensor_obs2008.csv.gz .
Open the Cloud Shell code editor.
Review the file send_sensor_data.py in the pubsub-streaming-lab folder.
On line 25, change the TOPIC variable from sandiego to your topic name.
Save the file.

Stream Data and Confirm Operation
Authenticate the shell with the following code:
gcloud auth application-default login
Click the generated link to confirm the authentication.
Execute the following command to install the Google Cloud Pub/Sub library:
sudo pip install -U google-cloud-pubsub
Execute the following code to simulate streaming data:
./send_sensor_data.py --speedFactor=60 --project=[PROJECT_ID]
Create a new Cloud Shell instance by clicking the plus (+) icon.
Change directory to the working folder:
cd training-data-analyst/courses/streaming/publish
Pull the messages from the subscription with the following command:
gcloud beta pubsub subscriptions pull --auto-ack [SUBSCRIPTION_NAME] --limit=25

----
Cloud VPC(virtual private cloud) Fundamentals
  Private network within Google Cloud network infrastructure
  Adds network to Compute Engine, Kubernetes Engine, and App Engine(Flex)
  Global resource consisting of regional subnets, connected by WAN
  Cloud VPC includes:
    Firewalls
    Routes
    Forwarding rules
    Configuration of IP addresses, external or internal
  Two types of VPC creation: Auto mode and Custom mode
  Additional services include Shared VPC and Network Peering

Cloud Load Balancing Fundamentals
  Fully managed incoming traffic service
  Distributes traffic across several VM instances
  Benefits:
    Autoscaling(by policy, CPU utilization, or serving capacity)
    Supports heavy traffic
    Routes traffic to closest instances
    Detect and remove unhealthy instances
  Types of load balancing supported:
    Global external- HTTP/S, SSL, and TCP
    Regional external- TCP/UDP within a region
    Regional internal- between groups of instances in a region

Cloud CDN Fundamentals
  Accelerates delivery from Compute Engine and Cloud Storage
  Lowers network latency, offloads origin servers, and reduces serving costs
  Features include:
    Offers SSL at no additional cost
    Supports HTTP/1.0, HTTP/1.1, HTTP/2
    Supports cache invalidation
    Cach-to-cache filling supported
  General availability caches to 10GB, Large Object Caching(Beta) to 5TB
  Caching considerations:
    Caching is reactive
    Caches cannot be preloaded
    Once enabled, caching is automatic
    HTTP(S) load balancer is required

Cloud VPN Fundamentals
  Provides secure connections between on-premises and Cloud VPC
  Utilizes IPsec VPN gateways with encrypted/decrypted traffic
  Features include:
    Site-to-site VPN via simple topology or with redundancy
    Suppports Internet Key Exchange(IKE) v1 and v2 with shared secret
    Uses ESP in Tunnel mode with authentication for encryption
  Routing methods supported:
    Dynamic gateways using Border Gateway Protocol
    Policy-based routing
    Route-based VPN
  Best for low/medium traffic: 3 GB/second with direct peering; 1.5 without

Cloud Interconnect Fundamentals
  Provides higher-capacity connections between on-prem and Cloud VPC
  Dedicated Interconnect
    Direct physical connections with Google Network
    69 colocations facilities in 17 regions
    Highest bandwidth: 10 GB/sec per circuit (8 circuits max)
    Routing equipment in colocation facility required
  Partner Interconnect
    Connects to 3rd party service provider
    Many more connection possibilities
    Bandwidth from 50 MB/sec to 10 GB/sec
    Routing equipment not required
  Public internet bypassed
  VPN tunnels or NAT devices not needed
  Not encrypted - use app level encryption or own VPN

----
Hands-On Lab: Build a Custom Network in Google Cloud Shell
Your team has a new internal app they want to start working on and you’ve
been asked to create the initial custom network with two subnets—one in
the US and another in Europe. You’ll also need to establish firewall rules
that allow SSH access and two Compute Engine instances, one of each of the
subnets. Finally, you’ll need to test the connectivity of the network from
the US to the European instances.

You’ll need to complete the following steps to accomplish your task:
Activate Cloud Shell
Click the Activate Cloud Shell icon at the top of the console page.

Create the Custom Network and Subnets
In the Cloud Shell, run the following command to create the network:
 gcloud compute networks create la-network --subnet-mode custom
To create the subnets, run the following commands:
 gcloud compute networks subnets create la-subnet-us-central --network la-network --region us-central1 --range 10.0.1.0/24
 gcloud compute networks subnets create la-subnet-eu-west --network la-network --region europe-west1 --range 10.0.2.0/24
List the created network by running the following command:
 gcloud compute networks subnets list --network la-network

Define the Firewall Rule
Create the desired firewall rule by running the following command:
 gcloud compute firewall-rules create la-allow-ssh --allow tcp:22,icmp --network la-network

 Spin Up the VM Instances
Create the Compute Engine instances by running the following commands:
 gcloud compute instances create la-vm-us --subnet la-subnet-us-central --zone us-central1-a
 gcloud compute instances create la-vm-eu --subnet la-subnet-eu-west --zone europe-west1-b

Test Via SSH
From the Compute Engine VM console page, click the SSH button for the la-vm-us instance to open its SSH terminal.
In the SSH terminal, run the following command to ping the VM instance in Europe:
 ping -c 3 [EUROPE_VM_EXTERNAL_IP]

----

Cloud AI Fundamentals
  Collection of services and APIs designed to facilitate machine learning
  Includes hardware accelerators: TPUs (Tensor Flow Processing Units)
  Primary service: Cloud Machine Learning Engine (ML Engine)
    Training
      Trains computer models to recognize patterns in data
      Supports TensorFlow, scikit-learn, and XGBoost
    Prediction
      Online
        Real-time processing with fully managed ML engines
        No Docker container required & supports multiple frameworks
      Batch
        For asynchronous operations
        Scales to terabytes of data
  Data must be stored in accessible location, e.g., Cloud Storage

Cloud IoT Core Fundamentals
  Fully managed service for connecting and managing IoT devices
  Devices must be registered
  Works with both telemetry (event data) & device state data
  Receives data and sends to Cloud Pub/Sub topic
  Supports HTTP and MQTT protocols for communication
  Highly secure:
    Each device uses JSON Web Tokens for public/private keys
    Supports RSA or Elliptic Curve algorithms to verify signatures
    Key rotation support
    Access to IoT core controlled by Cloud IAM roles and permissions
  Part of an IoT eco-system with Android Things and Google Beacon

Cloud Data Transfer Fundamentals
  Range of options available for transferring data to Google Cloud
  Online Transfer
    Tools available: console upload, JSON REST API, gsutil
  Storage Transfer Service
    Imports online data to Cloud Storage
    Supports transfer of objects from AWS S3
  Transfer Appliance
    Physical device loaded on-prem and shipped to Google data center
    Single device can hold petabyte of data
    Far faster than online transfer for large amounts of data

Hands-On Lab: Working with BigQuery in Google Cloud Shell
Your company is handling data analysis for a major metropolitan museum.
You’ve been asked to set up the preliminary BigQuery dataset and table and
load a large CSV file into the dataset. Naturally, you’ll need to run a
few queries to verify that the data has been successfully imported.

You’ll need to complete the following steps to accomplish your task:
Activate Cloud Shell
Click the Activate Cloud Shell icon at the top of the console page.

Retrieve the Data Files
In the Cloud Shell, run the following command:
 gsutil cp gs://la-gcp-labs-resources/essentials/MetObjects_BQ_Lab.csv .

Create a BigQuery Dataset
Create a BigQuery dataset with the following command:
 bq mk metobjects
List the current datasets with the following command:
 bq ls

Load the Data

Import the data into the dataset with the following command:
 bq load --source_format=CSV --skip_leading_rows=1 metobjects.linkedObjects MetObjects_BQ_Lab.csv title:STRING,artist:STRING,year:INTEGER,link:STRING
View the dataset with the following commands:
 bq ls metobjects
 bq show metobjects.linkedObjects

Query the Dataset
Execute a SQL query with the following command:
bq query "SELECT year,title,artist FROM metobjects.linkedObjects WHERE year > 2000 ORDER BY year DESC LIMIT 15"
Execute a second SQL query:
bq query "SELECT title,link FROM metobjects.linkedObjects WHERE year > 2000 ORDER BY year DESC LIMIT 15"
Test the available links.


